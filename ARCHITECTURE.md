# ğŸ—ï¸ Multi-Cloud Healthcare Application - Architecture Overview

## ğŸ“Œ Executive Summary

The **Multi-Cloud Healthcare Application** is a distributed, event-driven microservices system designed for managing healthcare workflows including patient records, appointments, billing, and real-time analytics. The system leverages a **hybrid multi-cloud architecture** across **AWS** and **Confluent Cloud (Kafka)**, with **Apache Flink** for stream processing.

---

## ğŸ¯ System Architecture

### **High-Level Architecture Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT LAYER                             â”‚
â”‚                     Web Portal (React + Vite)                    â”‚
â”‚              Port 80 (Nginx) - Hosted on AWS EKS                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ REST API (HTTPS) + WebSocket (WS)
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API GATEWAY / SERVICES LAYER                 â”‚
â”‚                         AWS EKS Cluster                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Auth Serviceâ”‚Patient Svc â”‚Appt Serviceâ”‚Billing Svc â”‚         â”‚
â”‚  â”‚  :3000     â”‚  :3001     â”‚  :3002     â”‚  :3003     â”‚         â”‚
â”‚  â”‚            â”‚            â”‚(WebSocket) â”‚            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚            â”‚            â”‚            â”‚
          â–¼            â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA PERSISTENCE LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ RDS/Postgresâ”‚  â”‚ DynamoDB   â”‚  â”‚ RDS/Postgresâ”‚  â”‚RDS/Postgresâ”‚
â”‚  â”‚  (Users)   â”‚  â”‚ (Patients) â”‚  â”‚(Appointments)â”‚  â”‚(Invoices)â”‚ â”‚
â”‚  â”‚ Auth Svc   â”‚  â”‚ Patient Svcâ”‚  â”‚  Appt Svc  â”‚  â”‚ Bill Svc â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚            â”‚
          â”‚                          â”‚            â”‚ Kafka Producer
          â”‚                          â”‚            â–¼
          â”‚                          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                          â”‚   â”‚ Confluent Cloud    â”‚
          â”‚                          â”‚   â”‚    Kafka Cluster   â”‚
          â”‚                          â”‚   â”‚                    â”‚
          â”‚                          â”‚   â”‚ Topics:            â”‚
          â”‚                          â”‚   â”‚ â€¢ appointment-eventsâ”‚
          â”‚                          â”‚   â”‚ â€¢ analytics-resultsâ”‚
          â”‚                          â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚         â”‚        â”‚
          â”‚                          â”‚         â”‚        â”‚ Consumer
          â”‚                          â”‚         â”‚        â–¼
          â”‚                          â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                          â”‚         â”‚   â”‚Appointment   â”‚
          â”‚                          â”‚         â”‚   â”‚  Service     â”‚
          â”‚                          â”‚         â”‚   â”‚ (WebSocket   â”‚
          â”‚                          â”‚         â”‚   â”‚  Broadcast)  â”‚
          â”‚                          â”‚         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚         â”‚
          â”‚                          â”‚         â”‚ Consumer
          â”‚                          â”‚         â–¼
          â”‚                          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                          â”‚   â”‚ Apache Flink Job â”‚
          â”‚                          â”‚   â”‚ (Java - Flink    â”‚
          â”‚                          â”‚   â”‚  1.18.0)         â”‚
          â”‚                          â”‚   â”‚                  â”‚
          â”‚                          â”‚   â”‚ Stream Processingâ”‚
          â”‚                          â”‚   â”‚ â€¢ Total Events   â”‚
          â”‚                          â”‚   â”‚ â€¢ Avg per Hour   â”‚
          â”‚                          â”‚   â”‚ â€¢ Windowing      â”‚
          â”‚                          â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚            â”‚
          â”‚                          â”‚            â”‚ Producer
          â”‚                          â”‚            â–¼
          â”‚                          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                          â”‚   â”‚ analytics-results  â”‚
          â”‚                          â”‚   â”‚      (Kafka)       â”‚
          â”‚                          â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚
          â”‚ Lambda Trigger           â”‚
          â–¼                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    AWS Lambda         â”‚           â”‚
â”‚ Invoice PDF Generator â”‚           â”‚
â”‚   (Node.js/Bun)       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (Invoked by Billing Service)
â”‚                       â”‚
â”‚  Uses:                â”‚
â”‚  â€¢ PDFKit             â”‚
â”‚  â€¢ AWS S3 SDK         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Upload PDF
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     AWS S3 Bucket     â”‚
â”‚ healthcare-lab-reportsâ”‚
â”‚   /invoices/*.pdf     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Service Architecture Breakdown

### **1. Auth Service** 
**Port:** `3000`  
**Technology Stack:** Bun + Hono + PostgreSQL  
**Purpose:** User authentication and authorization

#### **Responsibilities:**
- User registration with bcrypt password hashing
- User login with JWT token generation
- Token validation (JWT with 1-hour expiration)

#### **Data Storage:**
- **Database:** AWS RDS PostgreSQL
- **Table:** `users` (id, email, password_hash)

#### **API Endpoints:**
```
POST /register  - Register new user
POST /login     - Authenticate user and return JWT
GET  /          - Health check
```

#### **Environment Variables:**
```
DB_HOST, DB_USER, DB_PASSWORD, DB_NAME, JWT_SECRET
```

---

### **2. Patient Service**
**Port:** `3001`  
**Technology Stack:** Bun + Hono + AWS DynamoDB  
**Purpose:** Patient record management

#### **Responsibilities:**
- Create patient profiles
- Retrieve patient information
- Update patient records
- Scan all patients

#### **Data Storage:**
- **Database:** AWS DynamoDB
- **Table:** `patients`
- **Partition Key:** `id` (UUID)

#### **API Endpoints:**
```
GET    /patients      - List all patients
GET    /patients/:id  - Get patient by ID
POST   /patients      - Create new patient
PATCH  /patients/:id  - Update patient record
```

#### **Environment Variables:**
```
AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, TABLE_NAME
```

---

### **3. Appointment Service** â­
**Port:** `3002`  
**Technology Stack:** Bun + Hono + PostgreSQL + KafkaJS + WebSocket  
**Purpose:** Appointment scheduling and real-time analytics streaming

#### **Responsibilities:**
- Create and manage appointments
- **Publish events** to Kafka (`appointment-events` topic)
- **Consume analytics results** from Kafka (`analytics-results` topic)
- **Stream real-time analytics** to web clients via WebSocket

#### **Data Storage:**
- **Database:** AWS RDS PostgreSQL
- **Table:** `appointments` (id, patient_id, doctor_id, time)

#### **Event Architecture:**

**Producer Flow:**
```
POST /appointments 
  â†’ Save to DB 
  â†’ Publish to Kafka Topic: "appointment-events"
  â†’ Payload: { event, appointmentId, patientId, doctorId, time }
```

**Consumer Flow:**
```
Kafka Topic: "analytics-results" 
  â†’ Consumer (groupId: "appointment-service-group")
  â†’ Broadcast via WebSocket to all connected clients
```

#### **API Endpoints:**
```
GET    /appointments      - List all appointments
POST   /appointments      - Create appointment (triggers Kafka event)
WebSocket: ws://host:3002 - Real-time analytics stream
```

#### **WebSocket Payload Example:**
```json
{
  "metricType": "AppointmentAnalytics",
  "totalEventsCreated": 150,
  "avgAppointmentsPerHour": 12.5,
  "windowStartTime": 1701234567890,
  "windowEndTime": 1701238167890,
  "timestamp": 1701238167890
}
```

#### **Environment Variables:**
```
DB_HOST, DB_USER, DB_PASSWORD, DB_NAME
KAFKA_BROKERS, KAFKA_SASL_USERNAME, KAFKA_SASL_PASSWORD, KAFKA_SSL
```

---

### **4. Billing Service**
**Port:** `3003`  
**Technology Stack:** Bun + Hono + PostgreSQL + KafkaJS + AWS Lambda Client  
**Purpose:** Invoice generation and management

#### **Responsibilities:**
- **Consume appointment events** from Kafka
- Generate invoices with Lambda-generated PDFs
- Track invoice payment status
- Store invoice metadata with PDF URLs

#### **Data Storage:**
- **Database:** AWS RDS PostgreSQL
- **Table:** `invoices` (id, appointment_id, patient_id, amount, status, pdf_url, created_at)

#### **Event-Driven Workflow:**
```
Kafka "appointment-events" 
  â†’ Consumer (groupId: "billing-group")
  â†’ Calculate amount (based on doctorId)
  â†’ Invoke AWS Lambda ("lab_result_processor")
  â†’ Lambda generates PDF and uploads to S3
  â†’ Lambda returns pdfUrl
  â†’ Insert invoice record into DB with pdfUrl
```

#### **API Endpoints:**
```
GET    /invoices        - List all invoices
POST   /invoices/:id/pay - Mark invoice as PAID
```

#### **Lambda Integration:**
- **Function Name:** `lab_result_processor`
- **Payload:** `{ invoiceId, patientId, amount, date, status }`
- **Response:** `{ statusCode: 200, pdfUrl: "https://..." }`

#### **Environment Variables:**
```
DB_HOST, DB_USER, DB_PASSWORD, DB_NAME
KAFKA_BROKERS, KAFKA_SASL_USERNAME, KAFKA_SASL_PASSWORD, KAFKA_SSL
AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
```

---

### **5. Analytics Service** (Apache Flink)
**Technology Stack:** Java 11 + Apache Flink 1.18.0 + Kafka Connector  
**Purpose:** Real-time stream processing and analytics aggregation

#### **Responsibilities:**
- Consume appointment events from Kafka
- Calculate real-time metrics:
  - **Total events created** (cumulative counter)
  - **Average appointments per hour** (sliding window)
- Maintain stateful stream processing
- Publish analytics results back to Kafka

#### **Flink Job Architecture:**

**Data Flow:**
```
Kafka Source: "appointment-events"
  â†“
Parse JSON (AppointmentEvent POJO)
  â†“
KeyBy("global") - Single key for global state
  â†“
CumulativeMetricsFunction (Stateful Processing)
  - ValueState: cumulativeCount
  - ValueState: windowStart
  - ValueState: windowCount
  - Hourly window reset logic
  â†“
Create Analytics Result (JSON)
  â†“
Kafka Sink: "analytics-results"
```

**Processing Logic:**
1. **Cumulative Counter:** Increments on every event
2. **Hourly Rate Calculation:**
   - Tracks events within 1-hour window
   - Resets window after 3600000ms (1 hour)
   - Calculates: `avgPerHour = windowCount / elapsedHours`

**Output Schema:**
```json
{
  "metricType": "AppointmentAnalytics",
  "totalEventsCreated": 150,
  "avgAppointmentsPerHour": 12.5,
  "windowStartTime": 1701234567890,
  "windowEndTime": 1701238167890,
  "timestamp": 1701238167890
}
```

#### **Deployment:**
- **Execution Environment:** Standalone or cluster
- **Main Class:** `com.healthcare.streaming.HealthcareAnalyticsJob`
- **Build Tool:** Maven with Shade plugin (Fat JAR)

#### **Kafka Configuration:**
- **Bootstrap Servers:** `pkc-n3603.us-central1.gcp.confluent.cloud:9092`
- **Security:** SASL_SSL with PLAIN mechanism
- **Consumer Group:** `healthcare-analytics-group`
- **Offset Strategy:** Latest

---

### **6. Lab Result Processor (AWS Lambda)**
**Runtime:** Node.js (Bun/TypeScript compiled)  
**Technology Stack:** AWS Lambda + S3 SDK + PDFKit  
**Purpose:** Serverless invoice PDF generation

#### **Responsibilities:**
- Generate invoice PDFs dynamically
- Upload PDFs to S3 bucket
- Return S3 URL to caller (Billing Service)

#### **Workflow:**
```
Billing Service invokes Lambda
  â†“
Lambda receives payload: { invoiceId, patientId, amount, date, status }
  â†“
Generate PDF using PDFKit:
  - Invoice ID
  - Date
  - Patient ID
  - Amount Due
  - Status
  â†“
Upload to S3: s3://bucket/invoices/invoice_{id}.pdf
  â†“
Return: { statusCode: 200, pdfUrl: "https://..." }
```

#### **S3 Bucket Structure:**
```
healthcare-lab-reports-bucket/
  â””â”€â”€ invoices/
      â”œâ”€â”€ invoice_1.pdf
      â”œâ”€â”€ invoice_2.pdf
      â””â”€â”€ invoice_N.pdf
```

#### **Environment Variables:**
```
AWS_REGION, LAB_BUCKET_NAME
```

---

### **7. Web Portal**
**Port:** `80` (nginx serves static build)  
**Technology Stack:** React + Vite + TypeScript + TailwindCSS  
**Purpose:** Frontend UI for healthcare system

#### **Features:**
- User authentication (login/register)
- Patient management (CRUD)
- Appointment booking
- Billing/invoice management with PDF download
- **Real-time analytics dashboard** (WebSocket)

#### **Real-Time Analytics Integration:**

**WebSocket Connection:**
```typescript
const ws = new WebSocket(API_URLS.WS_APPOINTMENT);

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.metricType === 'AppointmentAnalytics') {
    setMetrics(data); // Update UI in real-time
  }
};
```

**Analytics Display:**
- **Total Events Created:** Cumulative counter
- **Average Appointments/Hour:** Sliding window metric

#### **API Configuration:**
All backend services are accessed via AWS LoadBalancer URLs (configured in `config.ts`)

---

## ğŸ”„ Data Flow Architecture

### **Flow 1: Appointment Creation â†’ Billing**

```
User (Web Portal)
  â†“ POST /appointments
Appointment Service
  â†“ Save to DB (appointments table)
  â†“ Publish event to Kafka
Kafka Topic: "appointment-events"
  â†“ Consumer 1: Billing Service
Billing Service
  â†“ Invoke Lambda (lab_result_processor)
AWS Lambda
  â†“ Generate PDF
  â†“ Upload to S3
  â†“ Return pdfUrl
Billing Service
  â†“ Insert invoice + pdfUrl into DB
```

### **Flow 2: Real-Time Analytics Pipeline**

```
Appointment Service
  â†“ Publish event to Kafka
Kafka Topic: "appointment-events"
  â†“ Consumer: Apache Flink Job
Flink Stream Processing
  â†“ Stateful aggregation (cumulative count + hourly avg)
  â†“ Publish result to Kafka
Kafka Topic: "analytics-results"
  â†“ Consumer: Appointment Service
Appointment Service
  â†“ Broadcast via WebSocket
Web Portal (React)
  â†“ Update UI in real-time
```

---

## â˜ï¸ Cloud Infrastructure

### **AWS Services Used:**

| Service | Purpose | Configuration |
|---------|---------|---------------|
| **EKS (Elastic Kubernetes Service)** | Container orchestration for microservices | Cluster: healthcare-cluster, Region: us-east-1 |
| **RDS PostgreSQL** | Relational database for Auth, Appointments, Billing | Multi-AZ deployment |
| **DynamoDB** | NoSQL database for Patient records | On-demand capacity |
| **S3** | Object storage for invoice PDFs | Bucket: healthcare-lab-reports-bucket |
| **Lambda** | Serverless PDF generation | Function: lab_result_processor |
| **ECR (Optional)** | Docker image registry | Alternative to Docker Hub |
| **Application Load Balancer** | Exposes services publicly | Type: LoadBalancer per service |

### **Confluent Cloud:**

| Resource | Configuration |
|----------|--------------|
| **Kafka Cluster** | Hosted on GCP (us-central1) |
| **Topics** | `appointment-events`, `analytics-results` |
| **Security** | SASL_SSL with API Key/Secret |
| **Bootstrap Server** | `pkc-n3603.us-central1.gcp.confluent.cloud:9092` |

---

## ğŸ³ Kubernetes Deployment

### **Deployment Strategy:**

All microservices are deployed to **AWS EKS** with:
- **Replicas:** 2 pods per service (High Availability)
- **Service Type:** LoadBalancer (External access)
- **Secrets Management:** Kubernetes Secrets (`app-secrets`)

### **Services & Ports:**

| Service | Replicas | Container Port | External Port | LoadBalancer URL |
|---------|----------|----------------|---------------|------------------|
| auth-service | 2 | 3000 | 80 | AWS ELB URL |
| patient-service | 2 | 3001 | 80 | AWS ELB URL |
| appointment-service | 2 | 3002 | 80 | AWS ELB URL |
| billing-service | 2 | 3003 | 80 | AWS ELB URL |
| web-portal | 2 | 80 | 80 | AWS ELB URL |

### **Environment Injection:**

All services receive configuration via Kubernetes Secrets:
- Database credentials (DB_HOST, DB_USER, DB_PASSWORD)
- Kafka credentials (KAFKA_BROKERS, KAFKA_SASL_USERNAME, KAFKA_SASL_PASSWORD)
- AWS credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
- JWT secrets

---

## ğŸ” Security Architecture

### **Authentication & Authorization:**
- **JWT (JSON Web Tokens):** 1-hour expiration
- **Bcrypt:** Password hashing (10 rounds)

### **Secrets Management:**
- **Kubernetes Secrets:** Encrypted at rest
- **Environment Variables:** Injected at runtime

### **Network Security:**
- **TLS/SSL:** Kafka uses SASL_SSL
- **CORS:** Enabled on all services

### **Database Security:**
- **RDS:** SSL connections, private subnets
- **DynamoDB:** IAM role-based access

---

## ğŸ“Š Observability & Monitoring

### **Logging:**
- **Approach:** Console logging in all services
- **Future Enhancement:** Integrate with AWS CloudWatch or ELK stack

### **Metrics:**
- **Kubernetes HPA:** Auto-scaling based on CPU/Memory (configured in k8s/hpa.yaml)
- **Flink Metrics:** Built-in Flink monitoring via web UI

### **Health Checks:**
- Each service exposes `GET /` for health monitoring

---

## ğŸ”§ Technology Stack Summary

| Layer | Technology |
|-------|-----------|
| **Runtime** | Bun (JavaScript/TypeScript) |
| **Web Framework** | Hono (lightweight HTTP framework) |
| **Frontend** | React + Vite + TailwindCSS |
| **Stream Processing** | Apache Flink 1.18.0 (Java) |
| **Message Broker** | Apache Kafka (Confluent Cloud) |
| **Databases** | PostgreSQL (RDS), DynamoDB |
| **Serverless** | AWS Lambda (Node.js) |
| **Container Orchestration** | Kubernetes (AWS EKS) |
| **Infrastructure as Code** | Terraform |
| **CI/CD** | Docker + Docker Hub |

---

## ğŸš€ Deployment Workflow

1. **Infrastructure Provisioning:** Terraform provisions AWS resources (EKS, RDS, DynamoDB, S3, Lambda)
2. **Build Docker Images:** Each service has a Dockerfile
3. **Push to Registry:** Docker Hub (laxmanlp777/*)
4. **Deploy to Kubernetes:** `kubectl apply -f k8s/deployments.yaml`
5. **Configure Secrets:** `kubectl apply -f k8s/secrets.yaml`
6. **Deploy Flink Job:** Submit JAR to Flink cluster or run standalone
7. **Access Application:** Via LoadBalancer URLs

---

## ğŸ“ˆ Scalability Design

### **Horizontal Scaling:**
- **Kubernetes HPA:** Auto-scales pods based on resource utilization
- **Stateless Services:** All microservices are stateless (except Flink state)
- **Database Scaling:** RDS read replicas, DynamoDB auto-scaling

### **Event-Driven Architecture:**
- **Decoupling:** Services communicate via Kafka (async)
- **Fault Tolerance:** Kafka retains events for replay
- **Load Distribution:** Multiple consumers in same group

### **Flink Scaling:**
- **Parallelism:** Configurable task parallelism
- **State Backend:** RocksDB for large state management

---

## ğŸ¯ Key Architectural Decisions

### **1. Why Bun instead of Node.js?**
- **Performance:** 3-4x faster startup time
- **Native TypeScript:** No build step needed for development
- **Compatibility:** Works with Node.js packages

### **2. Why DynamoDB for Patient Service?**
- **Flexibility:** Patient records may have varying attributes
- **Performance:** Single-digit millisecond latency
- **Scalability:** Automatic scaling without management

### **3. Why Apache Flink for Analytics?**
- **Stateful Processing:** Built-in support for windowing and aggregations
- **Exactly-Once Semantics:** Ensures accurate metrics
- **Low Latency:** Sub-second processing for real-time insights

### **4. Why WebSocket for Analytics?**
- **Real-Time Updates:** Push-based model (no polling)
- **Efficiency:** Single connection for continuous data flow
- **User Experience:** Live dashboard updates

### **5. Why Lambda for PDF Generation?**
- **Cost-Effective:** Pay only for execution time
- **Scalability:** Auto-scales with demand
- **Isolation:** Doesn't block billing service

---

## ğŸ” Data Models

### **appointments (PostgreSQL)**
```sql
CREATE TABLE appointments (
  id SERIAL PRIMARY KEY,
  patient_id VARCHAR(255),
  doctor_id VARCHAR(255),
  time TIMESTAMP
);
```

### **invoices (PostgreSQL)**
```sql
CREATE TABLE invoices (
  id SERIAL PRIMARY KEY,
  appointment_id VARCHAR(255),
  patient_id VARCHAR(255),
  amount DECIMAL(10,2),
  status VARCHAR(50),
  pdf_url TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);
```

### **users (PostgreSQL)**
```sql
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) UNIQUE,
  password_hash VARCHAR(255)
);
```

### **patients (DynamoDB)**
```json
{
  "id": "uuid-string",
  "name": "John Doe",
  "age": 35,
  "condition": "Hypertension",
  "...": "flexible schema"
}
```

---

## ğŸ”— Inter-Service Communication

### **Synchronous (REST):**
- Web Portal â†’ All Services (HTTP/HTTPS)
- Billing Service â†’ Lambda (AWS SDK)

### **Asynchronous (Event-Driven):**
- Appointment Service â†’ Kafka â†’ Billing Service
- Appointment Service â†’ Kafka â†’ Flink Analytics
- Flink Analytics â†’ Kafka â†’ Appointment Service

### **Real-Time (WebSocket):**
- Appointment Service â†’ Web Portal (Analytics Push)

---

## ğŸ“¦ Project Structure

```
cloud_assignment_15/
â”œâ”€â”€ analytics-service/          # Flink streaming job (Java)
â”‚   â”œâ”€â”€ src/main/java/...
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ appointment-service/        # Appointment + WebSocket (Bun)
â”‚   â”œâ”€â”€ index.ts
â”‚   â”œâ”€â”€ kafka.ts
â”‚   â””â”€â”€ db.ts
â”œâ”€â”€ auth-service/              # Authentication (Bun)
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ db.ts
â”œâ”€â”€ billing-service/           # Billing + Kafka consumer (Bun)
â”‚   â”œâ”€â”€ index.ts
â”‚   â”œâ”€â”€ kafka.ts
â”‚   â””â”€â”€ db.ts
â”œâ”€â”€ patient-service/           # Patient management (Bun)
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ db.ts
â”œâ”€â”€ lab-result-func/           # Lambda PDF generator (Node.js)
â”‚   â””â”€â”€ index.ts
â”œâ”€â”€ web-portal/                # React frontend
â”‚   â””â”€â”€ src/
â”œâ”€â”€ infrastructure/            # Terraform configs
â”‚   â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ gcp/
â”‚   â””â”€â”€ confluent/
â”œâ”€â”€ k8s/                      # Kubernetes manifests
â”‚   â”œâ”€â”€ deployments.yaml
â”‚   â”œâ”€â”€ secrets.yaml
â”‚   â””â”€â”€ hpa.yaml
â””â”€â”€ docker-push.sh            # Docker build/push script
```

---

## âœ… Current Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| Auth Service | âœ… Complete | JWT authentication working |
| Patient Service | âœ… Complete | DynamoDB integration working |
| Appointment Service | âœ… Complete | Kafka + WebSocket working |
| Billing Service | âœ… Complete | Lambda integration working |
| Analytics Service | âœ… Complete | Flink job processing events |
| Lambda PDF Generator | âœ… Complete | Generates and uploads to S3 |
| Web Portal | âœ… Complete | Real-time analytics dashboard |
| Kubernetes Deployment | âœ… Complete | All services on EKS |
| Kafka Integration | âœ… Complete | Confluent Cloud |

---

## ğŸ“ Learning Outcomes

This architecture demonstrates:
1. **Microservices Design Patterns:** Service decomposition, API gateway, event sourcing
2. **Event-Driven Architecture:** Kafka for async communication
3. **Stream Processing:** Apache Flink for real-time analytics
4. **Serverless Computing:** AWS Lambda for event-driven tasks
5. **Cloud-Native Development:** Kubernetes, containers, infrastructure as code
6. **Polyglot Persistence:** SQL, NoSQL, object storage
7. **Real-Time Communication:** WebSocket for live updates
8. **DevOps Practices:** Docker, K8s, automated deployments

---

## ğŸ”® Future Enhancements

1. **API Gateway:** Add Kong/AWS API Gateway for centralized routing
2. **Service Mesh:** Implement Istio for advanced traffic management
3. **Monitoring:** Integrate Prometheus + Grafana
4. **Logging:** Centralized logging with ELK stack
5. **Caching:** Redis for frequently accessed data
6. **CDN:** CloudFront for static assets
7. **CI/CD:** GitHub Actions or Jenkins pipeline
8. **Multi-Region:** Deploy across multiple AWS regions
9. **GraphQL:** Unified API layer
10. **Machine Learning:** Predictive analytics for patient care

---

## ğŸ“ Support & Documentation

- **Architecture Diagram:** See top of document
- **API Documentation:** Each service has inline comments
- **Deployment Guide:** See DEPLOYMENT.md (may be outdated)
- **Code Repository:** Current working directory

---

**Last Updated:** 2025-11-29  
**Architecture Version:** 2.0  
**System Status:** Production-Ready âœ…
